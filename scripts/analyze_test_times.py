"""
Analyze pytest JUnit XML output to find the slowest tests.

This script parses JUnit XML files generated by pytest --junitxml
and displays the top N slowest tests with their execution times.
"""

import argparse
import csv
import sys
import xml.etree.ElementTree as ET
from pathlib import Path


def parse_junit_xml(test_result_path):
    """Parse JUnit XML file and extract test information."""
    try:
        tree = ET.parse(test_result_path)
        root = tree.getroot()
    except ET.ParseError as e:
        print(f"Error parsing XML file: {e}")
        sys.exit(1)
    except FileNotFoundError:
        print(f"Error: File '{test_result_path}' not found")
        sys.exit(1)

    # handle both <testsuites> and direct <testsuite> root elements
    if root.tag == "testsuites":
        testsuites = root.findall("testsuite")
    elif root.tag == "testsuite":
        testsuites = [root]
    else:
        print(f"Error: Unexpected root element '{root.tag}'. Expected 'testsuites' or 'testsuite'")
        sys.exit(1)

    def strip_class_prefix(classname):
        """Strip common test prefix from class name for cleaner output."""
        prefix = "tests.core.pyspec.eth2spec.test."
        if classname.startswith(prefix):
            return classname[len(prefix) :]
        return classname

    tests = []
    for testsuite in testsuites:
        for testcase in testsuite.findall("testcase"):
            classname = strip_class_prefix(testcase.get("classname", "Unknown"))
            name = testcase.get("name", "Unknown")

            # check test status
            status = "passed"
            if testcase.find("failure") is not None:
                status = "failed"
            elif testcase.find("error") is not None:
                status = "error"
            elif testcase.find("skipped") is not None:
                status = "skipped"

            # get time
            time_str = testcase.get("time", "0")
            try:
                time = float(time_str)
            except ValueError:
                time = 0.0

            tests.append({"class": classname, "name": name, "status": status, "time": time})

    # sort tests by test time (descending)
    tests = sorted(tests, key=lambda t: t["time"], reverse=True)

    return tests


def save_to_csv(tests, save_csv_path):
    """Save test results to CSV file."""
    try:
        with save_csv_path.open(mode="w") as f:
            writer = csv.writer(f)
            writer.writerow(["Rank", "Time (s)", "Status", "Test Class", "Test Name"])

            # Write test data
            for i, test in enumerate(tests, 1):
                writer.writerow(
                    [
                        i,
                        f"{test['time']:.3f}",
                        test["status"],
                        test["class"],
                        test["name"],
                    ]
                )

        print(f"Results saved to: {save_csv_path}\n")
    except OSError as e:
        print(f"Error saving CSV file: {e}\n")


def display_test_time(tests):
    """Display the top N slowest tests."""

    print("=" * 80)
    print(f"{'Rank':>5} {'Time (s)':>10} {'Status':>8}   {'Test Class::Name'}")
    print("-" * 80)

    for i, test in enumerate(tests, 1):
        test_full_name = f"{test['class']}::{test['name']}"
        print(f"{i:>5} {test['time']:>10.3f} {test['status']:>8}   {test_full_name}")


def analyze_time_buckets(tests):
    """Analyze test counts by time buckets."""

    buckets = [
        (0, 1),
        (1, 10),
        (10, 60),
        (60, 300),
        (300, 600),
        (600, float("inf")),
    ]

    bucket_counts = {}
    total_tests = len(tests)
    for min_time, max_time in buckets:
        if max_time == float("inf"):
            label = f">{min_time}s"
            count = len([t for t in tests if t["time"] >= min_time])
        else:
            label = f"{min_time:>3}s - {max_time:>3}s"
            count = len([t for t in tests if min_time <= t["time"] < max_time])

        ratio = (count / total_tests) if total_tests > 0 else 0
        bucket_counts[label] = {"count": count, "ratio": ratio}

    return bucket_counts


def display_time_buckets(tests):
    """Display test count distribution by time buckets."""
    bucket_counts = analyze_time_buckets(tests)

    print("Test Distribution by Execution Time:")
    print("=" * 40)
    print(f"{'Time Range':<12} {'Count':<7} {'Percentage':<10}")
    print("-" * 40)

    for bucket_label, data in bucket_counts.items():
        count = data["count"]
        percentage = data["ratio"] * 100

        print(f"{bucket_label:<12} {count:>6,} {percentage:>10.2f}%")
    print()


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Analyze pytest JUnit XML output to find the slowest tests.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s
  %(prog)s --slowest 50
  %(prog)s --longer-than 180.0
  %(prog)s --save-csv""",
    )

    parser.add_argument(
        "--test-result-path",
        type=Path,
        default="tests/core/pyspec/test-reports/test_results.xml",
        help="Path to the JUnit XML file generated by pytest --junitxml (default: %(default)s)",
    )

    # Create mutually exclusive group for --slowest and --longer-than
    display_group = parser.add_mutually_exclusive_group()

    display_group.add_argument(
        "--slowest",
        type=int,
        default=20,
        metavar="N",
        help="Number of slowest tests to display",
    )

    display_group.add_argument(
        "--longer-than",
        type=float,
        metavar="SECONDS",
        help="Show all tests that take longer than SECONDS",
    )

    parser.add_argument(
        "--save-csv",
        type=Path,
        dest="save_csv_path",
        metavar="CSV_PATH",
        help="Save all test results to CSV file (default: %(default)s)",
    )

    return parser.parse_args()


def main():
    """Main function."""
    args = parse_arguments()

    # validate file exists
    if not args.test_result_path.exists():
        print(f"Error: File '{args.test_result_path}' does not exist. Run 'make test' first.")
        sys.exit(1)

    # parse the XML file
    tests = parse_junit_xml(args.test_result_path)
    if len(tests) == 0:
        print(f"Error: No test results found in '{args.test_result_path}'")
        sys.exit(1)

    # save to CSV if requested
    if args.save_csv_path:
        save_to_csv(tests, args.save_csv_path)

    print(f"Total tests: {len(tests):,}\n")

    # display time buckets
    display_time_buckets(tests)

    # display results
    if args.longer_than is not None:
        tests = [test for test in tests if test["time"] > args.longer_than]
        if not tests:
            print(f"No tests found that take longer than {args.longer_than} seconds.")
            sys.exit(0)
        else:
            print(f"Tests taking longer than {args.longer_than} seconds ({len(tests):,} found):")
    else:
        tests = tests[: args.slowest]
        print(f"Top {min(args.slowest, len(tests)):,} slowest tests:")
    display_test_time(tests)


if __name__ == "__main__":
    main()
